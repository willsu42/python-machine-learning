{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JY50zmDEa_q"
   },
   "source": [
    "# PART 2 TensorFlow\n",
    "# 6. Workshop 4-1 :  自然語言處理 (NLP) - LSTM Algorithm and its Application\n",
    "2019/08/30\n",
    "\n",
    "> [ Reference ] :\n",
    "1. Tom Hope, Yehezkel S. Resheff, and Itay Lieder, \"**`Learning TensorFlow : A Guide to Building Deep Learning Systems`**\", Chapter 5 & 6, O'Reilly, 2017.\n",
    "      [ Code ] : https://github.com/giser-yugang/Learning_TensorFlow\n",
    "2. Victor Zhou, \"**`An Introduction to Recurrent Neural Networks for Beginners`**\" Towards Data Science, 2019/07/25. https://towardsdatascience.com/an-introduction-to-recurrent-neural-networks-for-beginners-664d717adbd\n",
    "3. Andrej Karpathy, \"**`The Unreasonable Effectiveness of Recurrent Neural Networks`**\" Andrej Karpathy blog, 2015/05/21. http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "4. Wikipedia, \"**`Long short-term memory`**\", 2019. https://en.wikipedia.org/wiki/Long_short-term_memory\n",
    "5. 陳誠, \"**人人都能看懂的LSTM**\", https://zhuanlan.zhihu.com/p/32085405\n",
    "6. Wikipedia, \"**`Gated recurrent unit`**\" https://en.wikipedia.org/wiki/Gated_recurrent_unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Intro to LSTM Model](#Intro)\n",
    "- [1 One-Hot for Text Sequences](#OneHot)\n",
    "    - [1.1 Supervized Word Embeddings](#WordEmbeddings)\n",
    "    - [1.2 LSTM with Sequence Length](#LSTM)\n",
    "    - [1.3 Training Embeddings for the LSTM Classifier](#Training)\n",
    "    - [EXERCISE 1 : Stacking Multiple LSTMs (Solution)](#Ex1)\n",
    "- [2 Word2Vec for Text Sequences](#Word2Vec)\n",
    "    - [2.1 Skip-Grams](#SkipGram)\n",
    "    - [2.2 Building the Computation Graph](#BuildGraph)\n",
    "        - [Embeddings in TensorFlow](#Embeddings)\n",
    "        - [The Noise-Contrastive Estimation (NCE) Loss Function](#NCE)\n",
    "        - [Learning Rate Decay](#LRDecay)\n",
    "    - [2.3 Launching the Computation Graph](#LaunchGraph)\n",
    "        - [Checking Out the Embeddings](#CheckOut)\n",
    "    - [2.4 Visualization with TensorBoard](#TensorBoard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Intro'></a>\n",
    "# Intro to LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "![title](./Fig_1a_RNN_operation.png)\n",
    "\n",
    "**Figure 1.a RNN model.** (from Ref. 4)\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Long short-term memory (LSTM)** is *an artificial recurrent neural network (RNN) architecture* used in the field of deep learning. \n",
    "+ A common LSTM unit is composed of a `cell`, an `input gate`, an `output gate` and a `forget gate`. \n",
    "+ The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "![title](./Fig_1b_LSTM_model.png)\n",
    "\n",
    "**Figure 1.b The LSTM model.** (from Ref. 5)\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The basic ideas behind **LSTM models** :\n",
    "    + **Markov Chain Model**\n",
    "    + **Hidden Markov Model (HMM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "![title](./Fig_2a_LSTM_with_a_forget_gate.png)\n",
    "![title](./Fig_2b_LSTM_Variables_Activation_Funcs.png)\n",
    "\n",
    "----------------------\n",
    "**Figure 2 LSTM with a forget gate.** (from Ref. 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./Fig_3_Intro_to_RNNs.png)\n",
    "\n",
    "\n",
    "**Figure 3. Different Recurrent Neural Networks.** (from Ref. 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='OneHot'></a>\n",
    "## 1.  One-Hot for Text Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# for the old-version usage of TensorFlow, such as tensorflow.examples.tutorials.mnist\n",
    "old_v = tf.logging.get_verbosity()          \n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "batch_size = 128\n",
    "embedding_dimension = 64\n",
    "num_classes = 2\n",
    "hidden_layer_size = 32\n",
    "times_steps = 6 #1個timestep為一個字的sample\n",
    "element_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Next, we create sentences. We sample random digits and map them to the corresponding\n",
    "“words” (e.g., 1 is mapped to “One,” 7 to “Seven,” etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_to_word_map = {1:\"One\",2:\"Two\", 3:\"Three\", 4:\"Four\", 5:\"Five\",\n",
    "                     6:\"Six\",7:\"Seven\",8:\"Eight\",9:\"Nine\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Text sequences typically have variable lengths, which is of course the case for all real natural language data (such as in the sentences appearing on this page).\n",
    "\n",
    "> + To make our simulated sentences have different lengths, we sample for each sentence a random length between 3 and 6 with **`np.random.choice(range(3, 7))`**—the lower bound is inclusive, and the upper bound is exclusive.\n",
    "\n",
    "\n",
    "> + Now, to put all our input sentences in one tensor (per batch of data instances), we need them to somehow be of the same size—so we pad sentences with a length shorter than 6 with zeros (or PAD symbols) to make all sentences equally sized (artificially). This pre-processing step is known as **zero-padding**. \n",
    "    \n",
    "The following code accomplishes all of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_to_word_map[0]=\"PAD\" #dictionary加一個 0:PAD\n",
    "even_sentences = []\n",
    "odd_sentences = []\n",
    "seqlens = []\n",
    "\n",
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.choice(range(3,7)) #一句三到六個字\n",
    "    seqlens.append(rand_seq_len)\n",
    "    rand_odd_ints = np.random.choice(range(1,10,2),rand_seq_len)  \n",
    "    rand_even_ints = np.random.choice(range(2,10,2),rand_seq_len)\n",
    "    \n",
    "    # Padding\n",
    "    if rand_seq_len<6:\n",
    "        rand_odd_ints = np.append(rand_odd_ints,\n",
    "                                  [0]*(6-rand_seq_len))\n",
    "        rand_even_ints = np.append(rand_even_ints,\n",
    "                                  [0]*(6-rand_seq_len))\n",
    "    even_sentences.append(\" \".join([digit_to_word_map[r] \n",
    "                                    for r in rand_even_ints])) #空一格串起\n",
    "    odd_sentences.append(\" \".join([digit_to_word_map[r] \n",
    "                                   for r in rand_odd_ints]))\n",
    "    \n",
    "data = even_sentences+odd_sentences\n",
    "# Same seq lengths for even, odd sentences\n",
    "seqlens*=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at our sentences, each padded to length 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Six Six Two Eight Two Six',\n",
       " 'Six Six Eight Six Eight PAD',\n",
       " 'Eight Eight Six PAD PAD PAD',\n",
       " 'Six Six Eight Six Six Eight',\n",
       " 'Eight Six Two Four PAD PAD',\n",
       " 'Two Eight Two Eight PAD PAD']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_sentences[0:6]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One Seven Five One Nine One',\n",
       " 'Nine One Five Nine Nine PAD',\n",
       " 'Three Nine Seven PAD PAD PAD',\n",
       " 'One Three Five Three Seven Five',\n",
       " 'Three One Nine Three PAD PAD',\n",
       " 'Nine Nine Three Five PAD PAD']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_sentences[0:6]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> + Notice that we add the **PAD** word (token) to our data and `digit_to_word_map` dictionary, and separately store even and odd sentences and their original lengths (before padding).\n",
    "\n",
    "Let’s take a look at the original sequence lengths for the sentences we printed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 5, 3, 6, 4, 4]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqlens[0:6]  # Same seq lengths for even, odd sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q : Why keep the original sentence lengths? \n",
    "> + By zero-padding, we solved one technical\n",
    "problem but created another: if we naively pass these padded sentences through our\n",
    "RNN model as they are, it will process useless **PAD** symbols. \n",
    "\n",
    "\n",
    "> + This would both harm model correctness by processing “*noise*” and increase computation time. We resolve this issue by first storing the original lengths in the seqlens array and then telling TensorFlow’s **`tf.nn.dynamic_rnn()`** where each sentence ends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Our data is simulated—generated by us. In real applications, we would  start off by getting a collection of documents (e.g., one-sentence tweets) and then mapping each word to an integer ID.\n",
    "\n",
    "\n",
    "+ So, we now **map words to indices**—word `identifiers`—by simply creating a dictionary with words as keys and indices as values. \n",
    "+ We also create the **inverse map**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map from words to indices\n",
    "word2index_map ={}\n",
    "index=0\n",
    "for sent in data:\n",
    "    for word in sent.lower().split():  ##變小寫 分離（自動空格隔開）\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index+=1\n",
    "            \n",
    "# Inverse map\n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "vocabulary_size = len(index2word_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a supervised classification task—we need an array of labels in the `one-hot` format, train and test sets, a function to generate batches of instances, and placeholders, as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ First, we create the labels and split the data into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1]*10000 + [0]*10000\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot_encoding = [0]*2\n",
    "    one_hot_encoding[label] = 1\n",
    "    labels[i] = one_hot_encoding\n",
    "    \n",
    "data_indices = list(range(len(data)))\n",
    "np.random.shuffle(data_indices)\n",
    "data = np.array(data)[data_indices]\n",
    "\n",
    "labels = np.array(labels)[data_indices]\n",
    "seqlens = np.array(seqlens)[data_indices]\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "\n",
    "test_x = data[10000:]\n",
    "test_y = labels[10000:]\n",
    "test_seqlens = seqlens[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Next, we create a function that generates batches of sentences. Each sentence in a\n",
    "batch is simply a list of integer IDs corresponding to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_batch(batch_size,data_x, data_y, data_seqlens):\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [[word2index_map[word] for word in data_x[i].lower().split()]\n",
    "    for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    return x,y,seqlens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Finally, we create placeholders for data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inputs = tf.placeholder(tf.int32, shape=[batch_size,times_steps])\n",
    "_labels = tf.placeholder(tf.float32, shape=[batch_size, num_classes])\n",
    "\n",
    "# seqlens for dynamic calculation\n",
    "_seqlens = tf.placeholder(tf.int32, shape=[batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='WordEmbeddings'></a>   \n",
    "## 1.1 Supervised Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Word IDs encoded in `one-hot` (binary) categorical form**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.nn.embedding_lookup() function`\n",
    "+ Word embeddings as *basic hash tables* or *lookup tables*, mapping words to their dense vector values. These vectors are optimized as part of the training process.\n",
    "+ **Using the built-in `tf.nn.embedding_lookup()` function**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"embeddings\"):\n",
    "    embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size,\n",
    "                               embedding_dimension],\n",
    "                               -1.0, 1.0),name='embedding')\n",
    "    embed = tf.nn.embedding_lookup(embeddings, _inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LSTM'></a>  \n",
    "## 1.2 LSTM with Sequence Length\n",
    "\n",
    "+ A very popular recurrent network is the **`long short-term memory (LSTM) network`**. \n",
    ">    + It has some special *memory mechanisms* that enable the recurrent cells to better store information for long periods of time, thus allowing them to capture long-term dependencies better than plain RNN.\n",
    "    1. **These memory mechanisms simply consist of some more parameters added to each recurrent cell, enabling the RNN to overcome optimization issues and propagate information.** \n",
    "    2. **These trainable parameters act as filters that select what information is worth “*remembering*” and passing on, and what is worth “*forgetting*.”**\n",
    "+ **They are trained in exactly the same way as any other parameter in a network, with gradient-descent algorithms and backpropagation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating an LSTM cell with `tf.contrib.rnn.BasicLSTMCell()` and feed it to `tf.nn.dynamic_rnn()`:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size,\n",
    "                                              forget_bias=1.0)\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cell, embed,\n",
    "                                        sequence_length = _seqlens,\n",
    "                                        dtype=tf.float32)\n",
    "weights = {\n",
    "'linear_layer': tf.Variable(tf.truncated_normal([hidden_layer_size,\n",
    "                            num_classes], mean=0,stddev=.01))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "'linear_layer':tf.Variable(tf.truncated_normal([num_classes],\n",
    "                           mean=0,stddev=.01))\n",
    "}\n",
    "##最後output的weight和bias\n",
    "\n",
    "# Extract the last relevant output and use in a linear layer\n",
    "final_output = tf.matmul(states[1],\n",
    "                         weights[\"linear_layer\"]) + biases[\"linear_layer\"]\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(logits = final_output,\n",
    "                                                  labels = _labels)\n",
    "cross_entropy = tf.reduce_mean(softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **[ NOTE ] :  We take the last valid output vector — in this case conveniently available for us in the `states` tensor returned by `dynamic_rnn()` — and pass it through a linear layer (and the softmax function), using it as our final prediction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Training'></a>  \n",
    "## 1.3 Training Embeddings for the LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(_labels,1), tf.argmax(final_output,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at 0: 44.53125\n",
      "Accuracy at 100: 100.00000\n",
      "Accuracy at 200: 100.00000\n",
      "Accuracy at 300: 100.00000\n",
      "Accuracy at 400: 100.00000\n",
      "Accuracy at 500: 100.00000\n",
      "Accuracy at 600: 100.00000\n",
      "Accuracy at 700: 100.00000\n",
      "Accuracy at 800: 100.00000\n",
      "Accuracy at 900: 100.00000\n",
      "Test batch accuracy 0: 100.00000\n",
      "Test batch accuracy 1: 100.00000\n",
      "Test batch accuracy 2: 100.00000\n",
      "Test batch accuracy 3: 100.00000\n",
      "Test batch accuracy 4: 100.00000\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        x_batch, y_batch,seqlen_batch = get_sentence_batch(batch_size,\n",
    "                                         train_x,train_y, train_seqlens)\n",
    "        sess.run(train_step,feed_dict={_inputs:x_batch, _labels:y_batch,\n",
    "                                       _seqlens:seqlen_batch})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            acc = sess.run(accuracy,feed_dict={_inputs:x_batch,\n",
    "                                               _labels:y_batch,\n",
    "                                               _seqlens:seqlen_batch})\n",
    "            print(\"Accuracy at %d: %.5f\" % (step, acc))\n",
    "        \n",
    "    for test_batch in range(5):\n",
    "        x_test, y_test,seqlen_test = get_sentence_batch(batch_size,\n",
    "                                                        test_x,test_y,\n",
    "                                                        test_seqlens)\n",
    "        batch_pred,batch_acc = sess.run([tf.argmax(final_output,1), accuracy],\n",
    "                                         feed_dict={_inputs:x_test,\n",
    "                                                    _labels:y_test,\n",
    "                                                    _seqlens:seqlen_test})\n",
    "        print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc))\n",
    "    \n",
    "    output_example = sess.run([outputs],feed_dict={_inputs:x_test,\n",
    "                                                   _labels:y_test,\n",
    "                                                   _seqlens:seqlen_test})\n",
    "    states_example = sess.run([states[1]],feed_dict={_inputs:x_test,\n",
    "                                                     _labels:y_test,\n",
    "                                                     _seqlens:seqlen_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Let’s take a look at one example of these outputs, for a sentence that was zero-padded (in your random batch of data you may see different output, of course—look for a sentence whose seqlen was lower than the maximal 6):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqlen_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_example[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ This output has, as expected, six time steps, each a vector of size 32. \n",
    "\n",
    "Let’s take a glimpse at its values (printing only the first few dimensions to avoid clutter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37689742, -0.3723543 ,  0.25342464],\n",
       "       [ 0.61963177, -0.66885376,  0.52438927],\n",
       "       [ 0.7476994 , -0.8419353 ,  0.6733225 ],\n",
       "       [ 0.7789648 , -0.8884601 ,  0.68215954],\n",
       "       [ 0.7584404 , -0.872957  ,  0.72053736],\n",
       "       [ 0.        ,  0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_example[0][1][:6,0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ If a sentence has zero vectors in the last few time steps, it is because of zero-padding.\n",
    "\n",
    "Finally, we look at the states vector returned by `dynamic_rnn()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7584404 , -0.872957  ,  0.72053736], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_example[0][1][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **We can see that it conveniently stores for us the last relevant output vector — its values match the last relevant output vector before zero-padding.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------\n",
    "<a id='Ex1'></a>\n",
    "## [ EXERCISE 1 ] : Stacking multiple LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [ HINT ]: \n",
    "> 1. Using the **`MultiRNNCell()`** that combines multiple RNN cells into one multilayer cell.\n",
    "> 2. **The code segment in the Ref. 1 (Chapter 5) is incorrect. It should be modified as following:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building 2 LSTM layers\n",
    "num_LSTM_layers = 2\n",
    "\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cell = [tf.contrib.rnn.BasicLSTMCell(hidden_layer_size,forget_bias= 1.0) \n",
    "                 for _ in range(num_LSTM_layers)]\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells=lstm_cell,\n",
    "                                       state_is_tuple=True)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                        sequence_length = _seqlens,\n",
    "                                        dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ First, define an LSTM cell as before, and then feed it into the `tf.contrib.rnn.MultiRNNCell()` wrapper.\n",
    "\n",
    "+ Then, there are two layers of LSTM. \n",
    "+ To get the final state of the second layer, we simply adapt our indexing a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the final state and use in a linear layer\n",
    "final_output = tf.matmul(states[num_LSTM_layers-1][1],\n",
    "                         weights[\"linear_layer\"]) + biases[\"linear_layer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < Solution >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# for the old-version usage of TensorFlow, such as tensorflow.examples.tutorials.mnist\n",
    "old_v = tf.logging.get_verbosity()          \n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "batch_size = 128\n",
    "embedding_dimension = 64\n",
    "num_classes = 2\n",
    "hidden_layer_size = 32\n",
    "times_steps = 6\n",
    "element_size = 1\n",
    "\n",
    "# Creating index-to-word mapping\n",
    "digit_to_word_map = {1:\"One\",2:\"Two\", 3:\"Three\", 4:\"Four\", 5:\"Five\",\n",
    "                     6:\"Six\",7:\"Seven\",8:\"Eight\",9:\"Nine\"}\n",
    "\n",
    "digit_to_word_map[0]=\"PAD\"\n",
    "\n",
    "# Creating sentences for datasets\n",
    "even_sentences = []\n",
    "odd_sentences = []\n",
    "seqlens = []\n",
    "\n",
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.choice(range(3,7))\n",
    "    seqlens.append(rand_seq_len)\n",
    "    rand_odd_ints = np.random.choice(range(1,10,2),rand_seq_len)\n",
    "    rand_even_ints = np.random.choice(range(2,10,2),rand_seq_len)\n",
    "    \n",
    "    # Padding\n",
    "    if rand_seq_len<6:\n",
    "        rand_odd_ints = np.append(rand_odd_ints,\n",
    "                                  [0]*(6-rand_seq_len))\n",
    "        rand_even_ints = np.append(rand_even_ints,\n",
    "                                  [0]*(6-rand_seq_len))\n",
    "    even_sentences.append(\" \".join([digit_to_word_map[r] \n",
    "                                    for r in rand_even_ints]))\n",
    "    odd_sentences.append(\" \".join([digit_to_word_map[r] \n",
    "                                   for r in rand_odd_ints]))\n",
    "    \n",
    "data = even_sentences+odd_sentences\n",
    "\n",
    "# Same seq lengths for even, odd sentences\n",
    "seqlens*=2\n",
    "\n",
    "# Map from words to indices\n",
    "word2index_map ={}\n",
    "index=0\n",
    "for sent in data:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index+=1\n",
    "            \n",
    "# Inverse map\n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "vocabulary_size = len(index2word_map)\n",
    "\n",
    "# Arranging the train and test datasets\n",
    "labels = [1]*10000 + [0]*10000\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot_encoding = [0]*2\n",
    "    one_hot_encoding[label] = 1\n",
    "    labels[i] = one_hot_encoding\n",
    "    \n",
    "data_indices = list(range(len(data)))\n",
    "np.random.shuffle(data_indices)\n",
    "data = np.array(data)[data_indices]\n",
    "\n",
    "labels = np.array(labels)[data_indices]\n",
    "seqlens = np.array(seqlens)[data_indices]\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "\n",
    "test_x = data[10000:]\n",
    "test_y = labels[10000:]\n",
    "test_seqlens = seqlens[10000:]\n",
    "\n",
    "def get_sentence_batch(batch_size,data_x, data_y, data_seqlens):\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [[word2index_map[word] for word in data_x[i].lower().split()]\n",
    "    for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    return x,y,seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --------------------------------------\n",
    "## Building up the computation graph...\n",
    "## --------------------------------------\n",
    "# Need to clear the computational graph for creating 2 stacked LSTM layers\n",
    "tf.reset_default_graph()\n",
    "\n",
    "_inputs = tf.placeholder(tf.int32, shape=[batch_size,times_steps])\n",
    "_labels = tf.placeholder(tf.float32, shape=[batch_size, num_classes])\n",
    "\n",
    "# seqlens for dynamic calculation\n",
    "_seqlens = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "\n",
    "with tf.name_scope(\"embeddings\"):\n",
    "    embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size,\n",
    "                               embedding_dimension],\n",
    "                               -1.0, 1.0),name='embedding')\n",
    "    embed = tf.nn.embedding_lookup(embeddings, _inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating one LSTM cell with `tf.contrib.rnn.BasicLSTMCell()`, feed it into the `tf.contrib.rnn.MultiRNNCell()` and run it with `tf.nn.dynamic_rnn()`:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 2 stacked LSTM layers\n",
    "num_LSTM_layers = 2\n",
    "\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cell = [tf.contrib.rnn.BasicLSTMCell(hidden_layer_size,forget_bias= 1.0) \n",
    "                 for _ in range(num_LSTM_layers)]\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells=lstm_cell,\n",
    "                                       state_is_tuple=True)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                        sequence_length = _seqlens,\n",
    "                                        dtype=tf.float32)  ##兩層lstm\n",
    "    \n",
    "weights = {\n",
    "        'linear_layer':tf.Variable(tf.truncated_normal([hidden_layer_size,num_classes],\n",
    "                                                       mean=0,stddev=.01))\n",
    "}\n",
    "biases = {\n",
    "        'linear_layer':tf.Variable(tf.truncated_normal([num_classes],mean=0,stddev=.01))\n",
    "}\n",
    "\n",
    "## final_output = tf.matmul(states[1], weights['linear_layer']) + biases['linear_layer']\n",
    "# Extract the final state and use in a linear layer\n",
    "final_output = tf.matmul(states[num_LSTM_layers-1][1], weights[\"linear_layer\"]) + biases[\"linear_layer\"]\n",
    "\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(logits=final_output,labels=_labels)\n",
    "cross_entropy = tf.reduce_mean(softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ #### Training Embeddings and the Stacked LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(_labels,1), tf.argmax(final_output,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at 0: 60.93750\n",
      "Accuracy at 100: 100.00000\n",
      "Accuracy at 200: 100.00000\n",
      "Accuracy at 300: 100.00000\n",
      "Accuracy at 400: 100.00000\n",
      "Accuracy at 500: 100.00000\n",
      "Accuracy at 600: 100.00000\n",
      "Accuracy at 700: 100.00000\n",
      "Accuracy at 800: 100.00000\n",
      "Accuracy at 900: 100.00000\n",
      "Test batch accuracy 0: 100.00000\n",
      "Test batch accuracy 1: 100.00000\n",
      "Test batch accuracy 2: 100.00000\n",
      "Test batch accuracy 3: 100.00000\n",
      "Test batch accuracy 4: 100.00000\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        x_batch, y_batch,seqlen_batch = get_sentence_batch(batch_size,\n",
    "                                         train_x,train_y, train_seqlens)\n",
    "        sess.run(train_step,feed_dict={_inputs:x_batch, _labels:y_batch,\n",
    "                                       _seqlens:seqlen_batch})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            acc = sess.run(accuracy,feed_dict={_inputs:x_batch,\n",
    "                                               _labels:y_batch,\n",
    "                                               _seqlens:seqlen_batch})\n",
    "            print(\"Accuracy at %d: %.5f\" % (step, acc))\n",
    "        \n",
    "    for test_batch in range(5):\n",
    "        x_test, y_test,seqlen_test = get_sentence_batch(batch_size,\n",
    "                                                        test_x,test_y,\n",
    "                                                        test_seqlens)\n",
    "        batch_pred,batch_acc = sess.run([tf.argmax(final_output,1), accuracy],\n",
    "                                         feed_dict={_inputs:x_test,\n",
    "                                                    _labels:y_test,\n",
    "                                                    _seqlens:seqlen_test})\n",
    "        print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc))\n",
    "    \n",
    "    output_example = sess.run([outputs],feed_dict={_inputs:x_test,\n",
    "                                                   _labels:y_test,\n",
    "                                                   _seqlens:seqlen_test})\n",
    "    states_example = sess.run([states[1]],feed_dict={_inputs:x_test,\n",
    "                                                     _labels:y_test,\n",
    "                                                     _seqlens:seqlen_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "This output has 6 time steps, each with vector size 32 : (6, 32)\n",
      "\n",
      "Printing only the first 3 dimensions :\n",
      " [[-0.25823805  0.29683825  0.18983856]\n",
      " [-0.64487714  0.73287576  0.5071672 ]\n",
      " [-0.8181103   0.90600467  0.70263547]\n",
      " [-0.86785537  0.94548136  0.7704953 ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "\n",
      "\n",
      "2\n",
      "The states vector returned by dynamic_rnn() : \n",
      " [-0.86785537  0.94548136  0.7704953  -0.08993607  0.8896145   0.89738095\n",
      " -0.90628624 -0.8824731  -0.8189859  -0.5911037   0.8861079  -0.9172841\n",
      " -0.75333756 -0.9453625   0.9268746  -0.9108106   0.8645589   0.93709713\n",
      "  0.92868733 -0.8984062  -0.86088115  0.89710945 -0.93727446 -0.9186008\n",
      " -0.925568    0.9135656  -0.7785736   0.895155   -0.9355229  -0.9055909\n",
      "  0.51267844  0.9289578 ]\n"
     ]
    }
   ],
   "source": [
    "print(seqlen_test[1])\n",
    "print('This output has 6 time steps, each with vector size 32 :', output_example[0][1].shape)\n",
    "print('\\nPrinting only the first 3 dimensions :\\n', output_example[0][1][:6,:3])\n",
    "print('\\n')\n",
    "\n",
    "print(len(states_example[0]))\n",
    "print('The states vector returned by dynamic_rnn() : \\n', states_example[0][1][1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Word2Vec'></a>\n",
    "# 2. Word2Vec for Text Sequences\n",
    "> [ Reference ] :\n",
    "1. Tom Hope, Yehezkel S. Resheff, and Itay Lieder, \"**`Learning TensorFlow : A Guide to Building Deep Learning Systems`**\", Chapter 6, O'Reilly, 2017.\n",
    "\n",
    "+ skip-grams \n",
    "+ negative sampling \n",
    "+ **word embeddings** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# Define the hyperparameters\n",
    "batch_size = 64\n",
    "embedding_dimension=5\n",
    "negative_samples = 8\n",
    "LOG_DIR = 'word2vec'\n",
    "\n",
    "digit_to_word_map = {1: 'One', 2: 'Two', 3: 'Three', \n",
    "                     4: 'Four', 5: 'Five', 6: 'Six', \n",
    "                     7: 'Seven', 8: 'Eight', 9: 'Nine'}\n",
    "sentences = []\n",
    "\n",
    "# Create two kinds of sentences - sequences of odd and even digits\n",
    "for i in range(10000):\n",
    "    rand_odd_ints =np.random.choice(range(1,10,2),3)\n",
    "    sentences.append(' '.join([digit_to_word_map[r] for r in rand_odd_ints]))\n",
    "    rand_even_ints = np.random.choice(range(2,10,2),3)\n",
    "    sentences.append(' ' .join([digit_to_word_map[r] for r in rand_even_ints]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One Five One',\n",
       " 'Four Eight Eight',\n",
       " 'Five Nine One',\n",
       " 'Four Six Two',\n",
       " 'Seven Seven One',\n",
       " 'Six Two Six',\n",
       " 'One One Nine',\n",
       " 'Eight Two Four',\n",
       " 'Seven One Seven',\n",
       " 'Two Four Six']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map words to indices\n",
    "word2index_map = {}\n",
    "index=0\n",
    "for sent in sentences:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index+=1\n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "vocabulary_size = len(index2word_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='SkipGram'></a>\n",
    "### 2.1 Skip-Grams \n",
    "+ (Ref. 1, Chapter 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skipgram_batch(batch_size):\n",
    "    instance_indices = list(range(len(skip_gram_pairs)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [skip_gram_pairs[i][0] for i in batch]\n",
    "    y = [[skip_gram_pairs[i][1]] for i in batch]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map words to indices\n",
    "word2index_map = {}\n",
    "index = 0\n",
    "for sent in sentences:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index+=1\n",
    "index2word_map = {index:word for word,index in word2index_map.items()}\n",
    "vocabulary_size = len(index2word_map)\n",
    "\n",
    "#create skip-gram pairs\n",
    "skip_gram_pairs = []\n",
    "for sent in sentences:\n",
    "    tokenized_sent = sent.lower().split()\n",
    "    for i in range(1,len(tokenized_sent)-1):\n",
    "        word_context_pair = [[word2index_map[tokenized_sent[i-1]],\n",
    "                             word2index_map[tokenized_sent[i+1]]],\n",
    "                             word2index_map[tokenized_sent[i]]]\n",
    "        skip_gram_pairs.append([word_context_pair[1],\n",
    "                                word_context_pair[0][0]])\n",
    "        skip_gram_pairs.append([word_context_pair[1],\n",
    "                                word_context_pair[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0],\n",
       " [1, 0],\n",
       " [3, 2],\n",
       " [3, 3],\n",
       " [4, 1],\n",
       " [4, 0],\n",
       " [5, 2],\n",
       " [5, 6],\n",
       " [7, 7],\n",
       " [7, 0]]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram_pairs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nine', 'five', 'seven', 'four', 'six', 'four', 'nine', 'eight']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch example\n",
    "x_batch,y_batch = get_skipgram_batch(8)\n",
    "x_batch\n",
    "y_batch\n",
    "[index2word_map[word] for word in x_batch]\n",
    "[index2word_map[word[0]] for word in y_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 1, 4, 3, 2, 5, 7, 2]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4], [1], [7], [2], [5], [2], [4], [3]]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seven', 'five', 'nine', 'eight', 'four', 'six', 'seven', 'four']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[index2word_map[word] for word in x_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nine', 'five', 'seven', 'four', 'six', 'four', 'nine', 'eight']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[index2word_map[word[0]] for word in y_batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='BuildGraph'></a>\n",
    "### 2.2 Building the Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --------------------------------------\n",
    "## Building up the computation graph...\n",
    "## --------------------------------------\n",
    "# Need to reset the computational graph \n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Input data,labels\n",
    "train_inputs = tf.placeholder(tf.int32,shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32,shape=[batch_size,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Embeddings'></a>\n",
    "> ### Embeddings in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Here we use a loss function accounting for the *`unsupervised`* nature of the word-embedding task. \n",
    "+ Using the embedding lookup (the built-in **`tf.nn.embedding_lookup() function`**), which efficiently retrieves the vectors for each word in a given sequence of word indices, remains the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the lookup table\n",
    "with tf.name_scope('embeddings'):\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size,embedding_dimension],-1.0,1.0),name='embedding')\n",
    "    # This is essentially a lookup table\n",
    "    embed = tf.nn.embedding_lookup(embeddings,train_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='NCE'></a>\n",
    "> ### The Noise-Contrastive Estimation (NCE) Loss Function\n",
    ">    + `tf.nn.nce_loss()` automatically draws negative (“noise”) samples when we evaluate the loss (run it in a session):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'NCE_loss_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create variables for the NCE loss\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size,embedding_dimension],\n",
    "                                              stddev=1.0/math.sqrt(embedding_dimension)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights = nce_weights,\n",
    "                                     biases = nce_biases,\n",
    "                                     inputs = embed,\n",
    "                                     labels = train_labels,\n",
    "                                     num_sampled = negative_samples,\n",
    "                                     num_classes = vocabulary_size))\n",
    "\n",
    "tf.summary.scalar(\"NCE_loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LRDecay'></a>\n",
    "> ### Learning Rate Decay\n",
    ">    + **`tf.train.exponential_decay()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay\n",
    "global_step = tf.Variable(0,trainable=False)\n",
    "learningRate = tf.train.exponential_decay(learning_rate=0.1,\n",
    "                                          global_step = global_step,\n",
    "                                          decay_steps=1000,\n",
    "                                          decay_rate=0.95,\n",
    "                                          staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learningRate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LaunchGraph'></a>\n",
    "### 2.3 Launching the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss as 0: 00006\n",
      "loss as 100: 00003\n",
      "loss as 200: 00002\n",
      "loss as 300: 00002\n",
      "loss as 400: 00002\n",
      "loss as 500: 00002\n",
      "loss as 600: 00002\n",
      "loss as 700: 00002\n",
      "loss as 800: 00002\n",
      "loss as 900: 00002\n"
     ]
    }
   ],
   "source": [
    "# Merge all summary ops\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(LOG_DIR,graph=tf.get_default_graph())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with open(os.path.join(LOG_DIR,'metadata.tsv'),'w')as metadata:\n",
    "        metadata.write('Name\\tClass\\t\\n')\n",
    "        for k,v in index2word_map.items():\n",
    "            metadata.write('%s\\t%d\\t\\n' %(v,k))\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embeddings.name\n",
    "\n",
    "    # Link embedding to its metadata file\n",
    "    embedding.metadata_path = os.path.join(LOG_DIR,'metadata.tsv')\n",
    "    projector.visualize_embeddings(train_writer,config)\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(1000):\n",
    "        x_batch,y_bacth = get_skipgram_batch(batch_size)\n",
    "        summary,_ = sess.run([merged,train_step],feed_dict={train_inputs:x_batch,\n",
    "                                                            train_labels:y_bacth})\n",
    "        train_writer.add_summary(summary,step)\n",
    "        if step%100 ==0:\n",
    "            saver.save(sess,os.path.join(LOG_DIR,'w2c_model.skpt'),step)\n",
    "            loss_value = sess.run(loss,feed_dict={train_inputs:x_batch,\n",
    "                                                            train_labels:y_bacth})\n",
    "            print('loss as %d: %.5d'%(step,loss_value))\n",
    "\n",
    "    # Normalize embeddings before using\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings),1,keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    normalized_embeddings_matrix = sess.run(normalized_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='CheckOut'></a>\n",
    "> ### Checking Out the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three\n",
      "0.99160933\n",
      "five\n",
      "0.9637778\n",
      "seven\n",
      "0.9437641\n",
      "nine\n",
      "0.8734147\n",
      "four\n",
      "0.071836025\n",
      "two\n",
      "-0.03422053\n",
      "eight\n",
      "-0.036034882\n",
      "six\n",
      "-0.13121545\n"
     ]
    }
   ],
   "source": [
    "ref_word = normalized_embeddings_matrix[word2index_map['one']]\n",
    "cosine_dists = np.dot(normalized_embeddings_matrix,ref_word)\n",
    "ff = np.argsort(cosine_dists)[::-1][1:10]\n",
    "for f in ff:\n",
    "    print(index2word_map[f])\n",
    "    print(cosine_dists[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### [ Note ] :\n",
    ">+ The word vectors representing `odd numbers` are similar (in terms of the `dot product`) to `one`.\n",
    "+ Those representing `even numbers` are not similar to it (and have a `negative dot product` with the `one` vector).\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TensorBoard'></a>\n",
    "### 2.4 Visualization with TensorBoard\n",
    "\n",
    "> ####  To run TensorBoard, run the following command on `Anaconda Prompt` :\n",
    "`tensorboard --logdir=`_path/to/log-directory_\n",
    "\n",
    "> + For instance, **`tensorboard --logdir=C:\\DL\\logs\\word2vec`**\n",
    "\n",
    "> Connecting to **`http://localhost:6006`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "+ In TensorBoard, go to the `Projector` tab. This is a three-dimensional interactive visualization panel, where we can move around the space of our embedded vectors and explore different “angles,” zoom in, and more.\n",
    "\n",
    "\n",
    "+ **An `Embedding Projector` TensorFlow demo** : http://projector.tensorflow.org/\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Chapter5.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
